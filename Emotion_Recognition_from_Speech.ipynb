{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygkfDhXNugzd"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "693b760b"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists('./ravdess-emotional-speech-audio'):\n",
        "    os.makedirs('./ravdess-emotional-speech-audio')\n",
        "\n",
        "# Download the RAVDESS dataset (this link provides a combined zip for all actors)\n",
        "!wget -O ravdess-emotional-speech-audio.zip \"https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip?download=1\"\n",
        "\n",
        "# Unzip the downloaded file directly into the target directory\n",
        "!unzip -q ravdess-emotional-speech-audio.zip -d ./ravdess-emotional-speech-audio\n",
        "\n",
        "print(\"RAVDESS dataset downloaded and extracted.\")\n",
        "print(\"Listing contents of the extracted directory:\")\n",
        "!ls ./ravdess-emotional-speech-audio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "874e71d0"
      },
      "source": [
        "#Create a list of all audio file paths\n",
        "audio_files = []\n",
        "for root, dirs, files in os.walk('./ravdess-emotional-speech-audio'):\n",
        "\n",
        "    for file in files:\n",
        "        if file.lower().endswith('.wav'):\n",
        "            audio_files.append(os.path.join(root, file))\n",
        "\n",
        "print(\"Found files:\", len(audio_files))\n",
        "print(audio_files[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "088e7944"
      },
      "source": [
        "#Extract Labels\n",
        "emotion_map = {\n",
        "    '01':'neutral', '02':'calm', '03':'happy', '04':'sad',\n",
        "    '05':'angry', '06':'fearful', '07':'disgust', '08':'surprised'\n",
        "}\n",
        "\n",
        "labels = [emotion_map[os.path.basename(f).split('-')[2]] for f in audio_files]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db9e88ed"
      },
      "source": [
        "# Install resampy\n",
        "!pip install resampy\n",
        "#Feature Extraction (MFCCs)\n",
        "def extract_features(file):\n",
        "    audio, sample_rate = librosa.load(file, res_type='kaiser_fast')\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "    mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
        "    return mfccs_scaled\n",
        "\n",
        "# Only extract features if audio_files is not empty\n",
        "if audio_files:\n",
        "    features = np.array([extract_features(f) for f in audio_files])\n",
        "else:\n",
        "    features = np.array([])\n",
        "    print(\"No audio files found to extract features from.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Real-Time Prediction\n",
        "import sounddevice as sd\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "def record_audio(filename=\"test.wav\", duration=3, fs=44100):\n",
        "    print(\"Recording...\")\n",
        "    # In Colab, direct microphone access via sounddevice for real-time recording from user's local machine is not feasible.\n",
        "    # This function is retained for demonstration purposes if running locally, but will likely cause an error in Colab.\n",
        "    # For Colab, you would typically upload an audio file or use a browser-based recording solution.\n",
        "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
        "    sd.wait()\n",
        "    write(filename, fs, audio)\n",
        "    print(\"Saved:\", filename)\n",
        "\n",
        "print(\"Direct real-time recording from user's microphone is not supported in Colab backend.\")\n",
        "print(\"Instead, predicting emotion for an existing audio file from the dataset:\")\n",
        "print(\"Predicted Emotion:\", predict_emotion('./ravdess-emotional-speech-audio/Actor_01/03-01-03-01-01-01-01.wav'))"
      ],
      "metadata": {
        "id": "kxH3-kgDtVyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Augmentation (Audio Improve)\n",
        "def add_noise(data):\n",
        "    noise = np.random.randn(len(data))\n",
        "    return data + 0.005 * noise\n",
        "\n",
        "def pitch_shift(file):\n",
        "    y, sr = librosa.load(file)\n",
        "    return librosa.effects.pitch_shift(y, sr, n_steps=2)\n"
      ],
      "metadata": {
        "id": "DCP2UdtduhjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46b182b5"
      },
      "source": [
        "#Encode Labels\n",
        "le = LabelEncoder()\n",
        "# Only transform labels if the labels list is not empty\n",
        "if labels:\n",
        "    y = le.fit_transform(labels)\n",
        "    y = to_categorical(y)  # For multi-class classification\n",
        "else:\n",
        "    y = np.array([])\n",
        "    print(\"No labels found to encode.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of features:\", features.shape)"
      ],
      "metadata": {
        "id": "xx9EC4y-jtzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of encoded labels:\", y.shape)"
      ],
      "metadata": {
        "id": "WBHjDbTIjzA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "u4Uof8zbyWr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "bOn_U7gblZo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "18GPWIR7ldxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y portaudio19-dev\n",
        "!pip install sounddevice scipy"
      ],
      "metadata": {
        "id": "vARlWtZLtKnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the Model\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_shape=(40,), activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qoiIQVdRiaEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "aeYfnJ7sieK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Try Different Models\n",
        "X_cnn = X_train.reshape(-1, 40, 1, 1)\n",
        "X_test_cnn = X_test.reshape(-1, 40, 1, 1)"
      ],
      "metadata": {
        "id": "jdY7y2J7uBm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = Sequential([\n",
        "    Conv2D(32, (3,1), activation='relu', input_shape=(40,1,1)),\n",
        "    MaxPooling2D((2,1)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(y.shape[1], activation='softmax')\n",
        "])\n",
        "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn.fit(X_cnn, y_train, epochs=50, batch_size=32, validation_data=(X_test_cnn, y_test))"
      ],
      "metadata": {
        "id": "VkliedbauZcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot accuracy\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['accuracy'], label='Train')\n",
        "plt.plot(history.history['val_accuracy'], label='Test')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "SJ-MTfrdi2UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot loss\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Test')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "niJhWp_li-qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict New Audio\n",
        "def predict_emotion(file):\n",
        "    feature = extract_features(file)\n",
        "    feature = feature.reshape(1, -1)\n",
        "    prediction = model.predict(feature)\n",
        "    predicted_label = le.inverse_transform([np.argmax(prediction)])\n",
        "    return predicted_label[0]\n",
        "\n",
        "predict_emotion('./ravdess-emotional-speech-audio/Actor_01/03-01-03-01-01-01-01.wav')"
      ],
      "metadata": {
        "id": "J_Oe4jWojAAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ox_X7HCLn2AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true, y_pred_classes, target_names=le.classes_))"
      ],
      "metadata": {
        "id": "mWqD83h_nm9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Save\n",
        "model.save(\"emotion_model.h5\")"
      ],
      "metadata": {
        "id": "fV5uu60AoRLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Load\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(\"emotion_model.h5\")"
      ],
      "metadata": {
        "id": "O1YM7OnzobWn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}